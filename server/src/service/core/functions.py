# pip uninstall -y farm-haystack haystack-ai
# pip install haystack-ai
# pip install qdrant-haystack
# pip install "sentence-transformers>=2.2.0"
#

import os
import json
from src.database.mongodb.repository import mongo_client
from haystack.components.builders import DynamicChatPromptBuilder
# import gradio as gr
import pandas as pd
from haystack.document_stores.types import DuplicatePolicy
from haystack import Pipeline
from haystack.components.embedders import (
    SentenceTransformersTextEmbedder,
    SentenceTransformersDocumentEmbedder,
)
from haystack.components.generators import OpenAIGenerator
from haystack.dataclasses import ChatMessage, StreamingChunk
from haystack.components.generators.chat import OpenAIChatGenerator
from haystack import Document

# from load_documentstore import load_store
from haystack.dataclasses import ChatMessage
from haystack.components.generators.chat import OpenAIChatGenerator
from haystack_integrations.components.retrievers.qdrant import QdrantEmbeddingRetriever
from haystack.utils import Secret
from haystack_integrations.document_stores.qdrant import QdrantDocumentStore
import time
from haystack_integrations.components.embedders.cohere import (
    CohereDocumentEmbedder,
    CohereTextEmbedder,
)
from qdrant_client import QdrantClient

import time

file_path = "src/service/core/courses.csv"
url_cloud = (
    "https://f15cf5fc-0771-4b8a-aad5-c4f5c6ae1f1d.us-east4-0.gcp.cloud.qdrant.io:6333"
)
api_key = "U5tzMbWaGxk3wDvR9yzHCvnFVsTXosi5BR7qFcb7X_j7JOmo4L7RBA"

# index_name = "ThongTinKhoaHoc"
# model_name = "sentence-transformers/all-mpnet-base-v2"
# embedding_dim = 768
index_name = "ThongTinKhoaHoc_Cohere"
model_name = "embed-multilingual-v3.0"
# model_name = "intfloat/multilingual-e5-large-instruct"
embedding_dim = 1024


def load_collection():
    qdrant_client = QdrantClient(
        # url=url_cloud,
        # api_key=api_key,
        host= "qdrant",
    )

    return qdrant_client


# Create a new column which have content is name + description + skill
def add_content_current_course(filepath: str = file_path):
    df = pd.read_csv(filepath)
    df["content"] = (
        "Course Name: "
        + df["name"]
        + ".With course description: "
        + df["description"]
        + ". And this course will help you for improving skills such as: "
        + df["skills"]
    )
    course_info = dict(
        [
            (i, [x, y, z])
            for i, x, y, z in zip(
                df["name"].str.upper(), df["description"], df["skills"], df["content"]
            )
        ]
    )

    return course_info


# Call instance qdrant cloud
def load_store(
    index_name: str = index_name,
    url: str = url_cloud,
    token: str = api_key,
    embedding_dim: int = embedding_dim,
) -> QdrantDocumentStore:
    print(url)

    return QdrantDocumentStore(
        index=index_name,
        # url=url,
        # api_key=Secret.from_token(token=token),
        host = "qdrant",
        embedding_dim=embedding_dim,
    )
    # "m": 16,
    # Number of neighbours to consider during the index building. Larger the value - more accurate the search, more time required to build index.
    # "ef_construct": 100,
    # Minimal size (in KiloBytes) of vectors for additional payload-based indexing.
    # If payload chunk is smaller than `full_scan_threshold_kb` additional indexing won't be used -
    # in this case full-scan search should be preferred by query planner and additional indexing is not required.
    # Note: 1Kb = 1 vector of size 256

    # Number of parallel threads used for background index building. If 0 - auto selection.
    # "max_indexing_threads": 0,
    # Store HNSW index on disk. If set to false, index will be stored in RAM. Default: false


# Embed info
def embedding_csv(index_name: str = index_name, filepath: str = ".\courses.csv"):
    doc_store = load_store(
        index_name,
        url_cloud,
        api_key,
    )
    df = pd.read_csv(filepath)
    # Use data to initialize Document objects
    urls = list(df["url"].values)
    names = list(df["name"].values)
    instructors = list(df["instructor"].values)
    enrolls = list(df["enroll"].values)
    descriptions = list(df["description"].values)
    skills = list(df["skills"].values)
    relatives = list(df["relative"].values)
    rates = list(df["rate"].values)
    levels = list(df["level"].values)
    images = list(df["image"].values)
    df.fillna(value="", inplace=True)
    # init a document store
    docs = []
    for (
        url,
        name,
        instructor,
        enroll,
        description,
        skill,
        relative,
        rate,
        level,
        image,
    ) in zip(
        urls,
        names,
        instructors,
        enrolls,
        descriptions,
        skills,
        relatives,
        rates,
        levels,
        images,
    ):
        docs.append(
            Document(
                content=f"Course Name: {name}. How to access: {url} .With course description: {description}. And this course will help you for improving skills such as: {skill}",
                meta={
                    "name": name or "",
                    "url": url or "",
                    "instructor": instructor or "",
                    "enroll": enroll or "",
                    "skill": skill or "",
                    "relative": relative or "",
                    "rate": rate or "",
                    "level": level or "",
                    "image": image or "",
                },
            )
        )
    # init embedder
    doc_embedder = CohereDocumentEmbedder(model=model_name)
    # doc_embedder = SentenceTransformersDocumentEmbedder(model=model_name)
    # doc_embedder.warm_up()
    ## Use embedder Embedding file document for Fetch vÃ  Indexing
    docs_with_embeddings = doc_embedder.run(docs)
    doc_store.write_documents(
        docs_with_embeddings["documents"], policy=DuplicatePolicy.SKIP
    )
    if doc_store.count_documents() > 0:
        return "Success"
    else:
        return "Fail"


def get_current_collection():
    db = mongo_client["chatbot"]
    cur_collection = db["current_collection"]
    if cur_collection.count_documents({}) == 0:
        cur_collection.find_one_and_update(
            {"current_collection": "ThongTinKhoaHoc_Cohere"},
            {"$set": {"current_collection": "ThongTinKhoaHoc_Cohere"}},
            upsert=True,
        )
    cur_collection = db["current_collection"].find()[0]["current_collection"]
    return cur_collection


# RAG pipeline Q-A system
# def rag_pipe():
#     print(get_current_collection())
#     indexname = get_current_collection()
#     template = """
#     Answer the questions based on the given context. Don't use knowledge from outside. You are LearnWay Assistant bot, your purpose is to provide course information related to user's question. The course information should have name, how to access, skill. Answer with VietNamese language. The answer must be Comprehensive Information.

#     Context:
#     {% for document in documents %}
#         {{ document.content }}
#     {% endfor %}
#     Question: {{ question }}
#     Answer:
#     """
#     docstore = load_store(indexname)
#     if docstore.count_documents() == 0:
#         return 0
#     rag_pipe = Pipeline()
#     # rag_pipe.add_component(
#     #     "embedder", SentenceTransformersTextEmbedder(model=model_name)
#     # )
#     ranker = CohereRanker(api_key=Secret.from_token(api_cohere))
#     rag_pipe.add_component(
#         "embedder",
#         # SentenceTransformersTextEmbedder(model=model_name),
#         CohereTextEmbedder(Secret.from_token(api_cohere),model=model_name)
#     )
#     rag_pipe.add_component(
#         "retriever", QdrantEmbeddingRetriever(document_store=docstore, top_k=15)
#     )
#     rag_pipe.add_component(instance=ranker, name="ranker")
#     rag_pipe.add_component("prompt_builder", PromptBuilder(template=template))
#     rag_pipe.add_component("llm", OpenAIGenerator(model="gpt-3.5-turbo"))
#     rag_pipe.connect("embedder.embedding", "retriever.query_embedding")
#     rag_pipe.connect("retriever.documents", "ranker.documents")
#     rag_pipe.connect("ranker.documents", "prompt_builder.documents")
#     rag_pipe.connect("prompt_builder", "llm")

#     return rag_pipe



# RAG pipeline Q-A system
def rag_pipe():
    print(get_current_collection())
    indexname = get_current_collection()
    docstore = load_store(indexname)
    if docstore.count_documents() == 0:
        return 0
    rag_pipe = Pipeline()
    # rag_pipe.add_component(
    #     "embedder", SentenceTransformersTextEmbedder(model=model_name)
    # )
    # ranker = CohereRanker(model='rerank-multilingual-v3.0')
    rag_pipe.add_component(
        "embedder",
        # SentenceTransformersTextEmbedder(model=model_name),
        CohereTextEmbedder(model=model_name)
    )
    rag_pipe.add_component(
        "retriever", QdrantEmbeddingRetriever(document_store=docstore, top_k=10)
    )
    # rag_pipe.add_component(instance=ranker, name="ranker")
    # rag_pipe.add_component("prompt_builder", PromptBuilder(template=template))
    # rag_pipe.add_component("llm", OpenAIGenerator(model="gpt-3.5-turbo"))
    rag_pipe.connect("embedder.embedding", "retriever.query_embedding")
    # rag_pipe.connect("retriever.documents", "ranker.documents")
    # rag_pipe.connect("ranker.documents", "prompt_builder.documents")
    # rag_pipe.connect("prompt_builder", "llm")

    return rag_pipe

def rag_pipeline_func(query: str):
    start = time.time()
    if rag_pipe() == 0:
        return {"reply": "No data"}
    result = rag_pipe().run(
        {
            "embedder": {"text": query},
            # "ranker": {"query": query, "top_k": 10},
        }
    )
    content = ""
    x = result['retriever']['documents']
    for y in x:
        content= content + '\n' + y.content + ". "
    end = time.time()
    print("Rag time:", end - start)
    return {"reply": content}

# Run RAG Q-A system with query input
# def rag_pipeline_func(query: str):
#     start = time.time()
#     if rag_pipe() == 0:
#         return {"reply": "No data"}
#     result = rag_pipe().run(
#         {
#             "embedder": {"text": query},
#             "ranker": {"query": query, "top_k": 10},
#             "prompt_builder": {"question": query},
#         }
#     )
#     end = time.time()
#     print("Rag time:", end - start)
#     return {"reply": result["llm"]["replies"][0]}


# Get info (name + description + skill) through course name
def get_content_course(course_name: str, query="", filepath=file_path):
    course_info = add_content_current_course(filepath)
    if course_name.upper() in course_info:
        content = course_info[course_name.upper()][2]
        list_name = rag_pipeline_func(f"Get top 10 similar course of { content } ?")
        return list_name
    # fallback data
    else:
        return rag_pipeline_func(query)


def check(string):
    list = [". ", ": ", "- "]
    for i in list:
        if string.find(i) != -1:
            return i
    return ""


def check_and_strip_quotes(string):
    # Remove leading and trailing double quotes if they exist
    if string.startswith('"') and string.endswith('"'):
        return 1
    return 0


def get_suggestions(query: str, answer: str):
    start = time.time()
    language_classifier = OpenAIGenerator(model="gpt-3.5-turbo")
    language = language_classifier.run(
        f"LuÃ´n sá»­ dá»¥ng tiáº¿ng Viá»t vÃ  tráº£ káº¿t quáº£ lÃ  'vi'. Náº¿u cÃ³ yÃªu cáº§u sá»­ dá»¥ng má»t ngÃ´n ngá»¯ khÃ¡c, phÃ¡t hiá»n ngÃ´n ngá»¯ dá»±a vÃ o Äoáº¡n ná»i dung sau, náº¿u lÃ  tiáº¿ng anh thÃ¬ tráº£ káº¿t quáº£ 'en'. Äoáº¡n ná»i dung ÄÃ³ lÃ : {query}"
    )
    llm = OpenAIGenerator(model="gpt-3.5-turbo")
    # Function to detect the language of content
    if language["replies"][0] == "vi":  # Vietnamese
        # print("Há»i báº±ng tiáº¿ng Viá»t")
        response = llm.run(
            prompt = f"""
    Dá»±a trÃªn ngá»¯ cáº£nh sau, táº¡o danh sÃ¡ch cÃ¡c cÃ¢u há»i gá»£i Ã½ mÃ  ngÆ°á»i dÃ¹ng cÃ³ thá» muá»n há»i tiáº¿p theo. Táº¡o ra cÃ¡c cÃ¢u há»i gá»£i Ã½ cho ngÆ°á»i dÃ¹ng Äá» há»i há» thá»ng thÃ´ng qua chatbot. Má»i cÃ¢u há»i viáº¿t gá»n chá» dÆ°á»i 7 tá»«. Chá»§ 

    Ngá»¯ cáº£nh:
    CÃ¢u há»i: {query}. CÃ¢u tráº£ lá»i: {answer}

    CÃ¢u há»i gá»£i Ã½:
    1.
    2.
    3.
    4.
    """
        )
    else:  # Assuming English if not Vietnamese
        # print("Há»i báº±ng tiáº¿ng Anh")
        response = llm.run(
            f"""
    Given the following context, generate a list of suggested questions that the user might ask. Generate suggested questions for users to ask the system via chatbot. Each question has less than 7 words. 

    Context:
    Query: {query}. Answer: {answer}

    Suggested Questions:
    1.
    2.
    3.
    4.
    """ )
    list_of_lines = response["replies"][0].splitlines()
    # print(list_of_lines)
    clean_list = []
    char = check(list_of_lines[-1])
    if char != "":
        for i in list_of_lines:
            list = i.split(char)[1:]
            list = char.join(list)
            while check_and_strip_quotes(list) == 1:
                list = list[1:-1]
            clean_list.append(list)
        end = time.time()
        # print("Suggestion time: ",end - start)
        return clean_list[-4:]
    for i in list_of_lines:
        while check_and_strip_quotes(i) == 1:
            i = i[1:-1]
        clean_list.append(i)
    end = time.time()
    # print("Suggestion time: ",end - start)
    return clean_list[-4:]


def get_summarize_chat(query: str):
    start = time.time()
    llm = OpenAIGenerator(model="gpt-3.5-turbo")
    response = llm.run(
        f"Sá»­ dá»¥ng tiáº¿ng Viá»t vÃ  tÃ³m táº¯t chá»§ Äá» vÃ  hiá»u Ã½ Äá»nh mong muá»n cá»§a user tá»« cÃ¢u há»i báº±ng má»t cÃ¢u dÆ°á»i 10 tá»«. KhÃ´ng cáº§n chá»§ ngá»¯. CÃ¢u há»i ÄÃ³ lÃ : {query}"
    )
    summary = response["replies"][0]
    char = check(summary)
    if char != "":
        summary = summary.split(char)[-1]
        while check_and_strip_quotes(summary) == 1:
            summary = summary[1:-1]
        return summary
    while check_and_strip_quotes(summary) == 1:
        summary = summary[1:-1]
    end = time.time()
    # print("summarize time: ",end - start)
    return summary


def get_career_skills(
    goal_career: str,
    current_career: str,
    current_skills: str,
    goal_skills: str,
    query: str,
):
    list_of_current_skills = current_skills.split(", ")
    list_of_goal_skills = goal_skills.split(", ")
    if goal_career != None or goal_career != "":

        # call recommendation career path function with above inputs

        print(goal_career)
        print(goal_skills)
        print(current_skills)
        print(list_of_current_skills)
        print(list_of_goal_skills)
        print(current_career)

        return {"reply": "Hiá»n thÃ´ng tin cho lá» trÃ¬nh chÆ°a ÄÆ°á»£c cung cáº¥p. HÃ£y thá»­ láº¡i vÃ o láº§n sau!"}
    # fallback data
    else:
        return rag_pipeline_func(query)


def chatbot_with_fc(message, messages=[]):
    start = time.time()
    name_chat = ""
    if messages == []:
        name_chat = ""
        # get_summarize_chat(message)
    if message == []:
        messages.append(
            ChatMessage.from_system(
                "Náº¿u khÃ´ng cÃ³ yÃªu cáº§u chuyá»n ngÃ´n ngá»¯ tá»« user, thÃ¬ luÃ´n tráº£ lá»i báº±ng tiáº¿ng viá»t. Náº¿u ngÃ´n ngá»¯ cá»§a user lÃ  tiáº¿ng viá»t thÃ¬ luÃ´n tráº£ lá»i báº±ng tiáº¿ng Viá»t. Báº¡n chá» tráº£ lá»i dá»±a trÃªn thÃ´ng tin ÄÆ°á»£c cung cáº¥p, khÃ´ng ÄÆ°á»£c tá»± láº¥y thÃ´ng tin ngoÃ i Äá» tráº£ lá»i cho user. VÃ  Äá»nh dáº¡ng hÃ¬nh thá»©c tráº£ lá»i sao cho Äáº¹p."
            )
        )

    tools = [
        {
            "type": "function",
            "function": {
                "name": "rag_pipeline_func",
                "description": "Get information",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "query": {
                            "type": "string",
                            "description": "The query to use in the search. Infer this from the user's message. It should be a question or a statement",
                        }
                    },
                    "required": ["query"],
                },
            },
        },
        {
            "type": "function",
            "function": {
                "name": "get_content_course",
                "description": "Get similarity courses",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "course_name": {
                            "type": "string",
                            "description": "Name of the course, e.g. Applied Software Engineering Fundamentals Specialization, AI Foundations for Everyone Specialization",
                        },
                        "query": {
                            "type": "string",
                            "description": "The query to use in the search. Infer this from the user's message. It should be a question or a statement",
                        },
                    },
                    "required": ["course_name", "query"],
                },
            },
        },
        {
            "type": "function",
            "function": {
                "name": "get_career_skills",
                "description": "Get user's goal and current career and get user's goal and current skills",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "goal_career": {
                            "type": "string",
                            "description": "Name of user's goal career, e.g. Backend Developer, Business Analyst, Data Analysts, Data Engineer, Data Scientist, Database Administrator,Devops Engineer,Frontend Developer,Game Development,Mobile Developer",
                        },
                        "current_career": {
                            "type": "string",
                            "description": "Name of user's current career, e.g. Backend Developer, Business Analyst, Data Analysts, Data Engineer, Data Scientist, Database Administrator,Devops Engineer,Frontend Developer,Game Development,Mobile Developer",
                        },
                        "goal_skills": {
                            "type": "string",
                            "description": "List name of user's goal skills, e.g. power bi, ssis,sql server,mysql ,redis ,docker ,software product management,.net core framework ,github, object-oriented programming (oop) ,relational database management systems (rdbms),data visualization,data warehouse,graphql,java,javascript ,machine learning,data analysis,business intelligence ,r,python , sql,golang...",
                        },
                        "current_skills": {
                            "type": "string",
                            "description": "List name of user's current skills, e.g. power bi, ssis,sql server,mysql ,redis ,docker ,software product management,.net core framework ,github, object-oriented programming (oop) ,relational database management systems (rdbms),data visualization,data warehouse,graphql,java,javascript ,machine learning,data analysis,business intelligence ,r,python , sql,golang...",
                        },
                        "query": {
                            "type": "string",
                            "description": "The query to use in the search. Infer this from the user's message. It should be a question or a statement",
                        },
                    },
                    "required": [
                        "goal_career",
                        "current_career",
                        "goal_skills",
                        "current_skills",
                        "query",
                    ],
                },
            },
        },
    ]

    messages.append(ChatMessage.from_user(message))
    chat_generator = OpenAIChatGenerator(model="gpt-3.5-turbo")
    messages.append(
        ChatMessage.from_system(
            "Tráº£ lá»i ngáº¯n gá»n Äá»§ Ã½. KhÃ´ng ÄÆ°á»£c tá»± suy luáº­n thiáº¿u thÃ´ng tin tá»« dá»¯ liá»u chat. Náº¿u cÃ¢u tráº£ lá»i cá»§a báº¡n thÃ´ng bÃ¡o khÃ´ng cÃ³ thÃ´ng tin hoáº·c cáº§n cung cáº¥p thÃ´ng tin thÃ´ng tin thÃ¬ báº¡n pháº£i truyá»n giÃ¡ trá» cho finish_reason lÃ  'tool_calls', khÃ´ng sá»­ dá»¥ng thÃ´ng tin tá»« bÃªn ngoÃ i. Náº¿u khÃ´ng cÃ³ yÃªu cáº§u chuyá»n ngÃ´n ngá»¯ tá»« user, thÃ¬ luÃ´n tráº£ lá»i báº±ng tiáº¿ng viá»t. Náº¿u ngÃ´n ngá»¯ cá»§a user lÃ  tiáº¿ng viá»t thÃ¬ luÃ´n tráº£ lá»i báº±ng tiáº¿ng Viá»t. Báº¡n chá» tráº£ lá»i dá»±a trÃªn thÃ´ng tin ÄÆ°á»£c cung cáº¥p, khÃ´ng ÄÆ°á»£c tá»± láº¥y thÃ´ng tin ngoÃ i Äá» tráº£ lá»i cho user. Cáº§n Äá»nh dáº¡ng hÃ¬nh thá»©c cÃ¢u tráº£ lá»i sao cho rÃµ rÃ ng vÃ  Äáº¹p."
        )
    )
    start2 = time.time()
    
    response = chat_generator.run(messages=messages, generation_kwargs={"tools": tools})
    print("First: ",  time.time() - start2)
    # llm = OpenAIGenerator(model="gpt-3.5-turbo")
    # check_tool = llm.run(
    #     f"Náº¿u ná»i dung cá»§a cÃ¢u tráº£ lá»i cÃ³ Ã½ thiáº¿u thÃ´ng tin hoáº·c cáº§n cung cáº¥p thÃ´ng tin tá»« ngÆ°á»i dÃ¹ng thÃ¬ tráº£ lá»i lÃ  'yes', ngÆ°á»£c láº¡i tráº£ káº¿t quáº£ 'no'. CÃ¢u tráº£ lá»i nhÆ° sau: {response['replies'][0].content}. "
    # )
    # print(check_tool["replies"][0])
    # kq = check_tool["replies"][0]
    while True:
        # if OpenAI response is a tool call
        print(response)
        print(response["replies"][0].meta["finish_reason"])
        temp = response["replies"][0].meta["finish_reason"]
        if temp == "tool_calls":
            list_func_call = json.loads(response["replies"][0].content)
        print(str({"query": message}))
        # if kq.lower() == "yes":
        #     temp = "tool_calls"
        #     list_func_call = [
        #         {
        #             "function": {
        #                 "name": "rag_pipeline_func",
        #                 "arguments": json.dumps({"query": message}),
        #             }
        #         }
        #     ]
        # kq = "no"
        if response and temp == "tool_calls":
            function_calls = list_func_call
            print(function_calls)
            for function_call in function_calls:
                ## Parse function calling information
                function_name = function_call["function"]["name"]
                function_args = json.loads(function_call["function"]["arguments"])
                available_functions = {
                    "rag_pipeline_func": rag_pipeline_func,
                    "get_content_course": get_content_course,
                    "get_career_skills": get_career_skills,
                }
                ## Find the corresponding function and call it with the given arguments
                function_to_call = available_functions[function_name]
                function_response = function_to_call(**function_args)

                ## Append function response to the messages list using `ChatMessage.from_function`
                messages.append(
                    ChatMessage.from_function(
                        content=json.dumps(function_response), name=function_name
                    )
                )
                start3 = time.time()
                response = chat_generator.run(
                    messages=messages, generation_kwargs={"tools": tools}
                )
            
                messages.pop()
                print("Second: ",  time.time() - start3)
        # Regular Conversation
        else:
            messages.append(response["replies"][0])
            break
        # get suggestions for user to ask

    suggestions = ""
    # get_suggestions(message, response["replies"][0].content)
    print(response["replies"][0].content)
    end = time.time()
    print("chat time: ",end - start)
    return {
        "history": messages,
        "answer": response["replies"][0].content,
        "tag": suggestions,
        "name_chat": name_chat,
    }


def chatbot_pipeline(query:str, history = []):
    start = time.time()
    document_store = load_store(index_name=get_current_collection())
    print(get_current_collection())
    if document_store.count_documents() == 0:
        messages = history
        messages.append(ChatMessage.from_function(content=f"KhÃ´ng cÃ³ dá»¯ liá»u trong há» thá»ng cho cÃ¢u há»i : {query}",name="chatbot_pipeline"))
    else:     
        pipeline = Pipeline()
        pipeline.add_component("embedder", CohereTextEmbedder(model=model_name))
        pipeline.add_component("retriever", QdrantEmbeddingRetriever(document_store=document_store))
        pipeline.add_component("prompt_builder", DynamicChatPromptBuilder(runtime_variables=["query", "documents"]))
        # pipeline.add_component("llm", OpenAIChatGenerator(model= "gpt-3.5-turbo"))
        pipeline.connect("embedder.embedding", "retriever.query_embedding")
        pipeline.connect("retriever.documents", "prompt_builder.documents")
        # pipeline.connect("prompt_builder.prompt", "llm.messages")
        system_message =    ChatMessage.from_system(
                f"""Náº¿u thÃ´ng tin cÃ³ ÄÆ°á»ng dáº«n (web link) kÃ¨m theo thÃ¬ pháº£i thÃªm vÃ o cÃ¢u tráº£ lá»i, náº¿u khÃ´ng cÃ³ thÃ¬ khÃ´ng ÄÆ°á»£c tá»± Ã½ ÄÆ°á»ng dáº«n khÃ´ng cÃ³ trong dá»¯ liá»u. Tráº£ lá»i ngáº¯n gá»n Äá»§ Ã½. KhÃ´ng ÄÆ°á»£c tá»± suy luáº­n thiáº¿u thÃ´ng tin tá»« dá»¯ liá»u chat.
                ThÃªm ngá»¯ cáº£nh cho cÃ¢u há»i dá»±a vÃ o cÃ¡c cÃ¢u há»i trÆ°á»c cá»§a user. Náº¿u thiáº¿u thÃ´ng tin cáº§n gá»i hÃ m Äá» láº¥y thÃªm thÃ´ng tin.
                Báº¡n chá» tráº£ lá»i dá»±a trÃªn thÃ´ng tin ÄÆ°á»£c cung cáº¥p, khÃ´ng ÄÆ°á»£c tá»± láº¥y thÃ´ng tin ngoÃ i Äá» tráº£ lá»i cho user.
                Náº¿u khÃ´ng cÃ³ yÃªu cáº§u chuyá»n ngÃ´n ngá»¯ tá»« user, thÃ¬ luÃ´n tráº£ lá»i báº±ng tiáº¿ng viá»t. Náº¿u ngÃ´n ngá»¯ cá»§a user lÃ  tiáº¿ng viá»t
                thÃ¬ luÃ´n tráº£ lá»i báº±ng tiáº¿ng Viá»t.
                Cáº§n Äá»nh dáº¡ng hÃ¬nh thá»©c cÃ¢u tráº£ lá»i sao cho rÃµ rÃ ng vÃ  Äáº¹p. """
            )

        history.append(system_message)
        history.append(ChatMessage.from_user("""
        Given these documents , answer the question. If data has link, must add link into the answer, else don't need to create to add. Bold the numbering and the content within the list items to enhance readability and organization. \nDocuments:
            {% for doc in documents %}
                {{ doc.content }}
            {% endfor %}

            \nQuestion: {{query}}
            \nAnswer:
        """))
        messages = pipeline.run(data={"embedder": {"text": query}, "prompt_builder": { "prompt_source": history, "query": query}})['prompt_builder']['prompt']
    chat_generator = OpenAIChatGenerator(model= "gpt-3.5-turbo")
        # print(messages)
        # print("CÃ¢u há»i nÃ¨: ",query)
    return chat_generator.client.chat.completions.create(
                model=chat_generator.model,
                messages=[mess.to_openai_format() for mess in messages],            
                stream=True )



def chatbot_with_fc_stream(query:str, history = []):
    tools = [
                {
                    "type": "function",
                    "function": {
                        "name": "chatbot_pipeline",
                        "description": "Get any information",
                        "parameters": {
                            "type": "object",
                            "properties": {
                                "query": {
                                    "type": "string",
                                    "description": "The query to use in the search. Infer this from the user's message. It should be a question or a statement. Add context for the query.",
                                }
                            },
                            "required": ["query"],
                        },
                    },
                },]
    chat_generator = OpenAIChatGenerator(model="gpt-3.5-turbo")
    
    
    # prompt= f"""Using Vietnamese. Create a SINGLE standalone question. If the question lacks a subject and context, the question should be based on the New question plus the previous query and answer is : {history[-4:-3]}. 
    # If the New question includes a subject and sufficient context and can stand on its own you should return the New question {query}. New question is: \"{query}\""""
    # prompt = f"""Sá»­ dá»¥ng tiáº¿ng viá»t, táº¡o má»t cÃ¢u há»i ngáº¯n gá»n Äá»§ Ã½, náº¿u cÃ¢u há»i má»i thiáº¿u chá»§ ngá»¯ thÃ¬ dá»±a trÃªn cÃ¡c cÃ¢u há»i cÅ© liá»n ká» á» trÆ°á»c Äá» tÃ¬m chá»§ ngá»¯ vÃ  bá» sung chá»§ ngá»¯ nÃ y vÃ o cÃ¢u há»i má»i '{query}' vÃ  tráº£ káº¿t quáº£ lÃ  cÃ¢u há»i má»i ÄÆ°á»£c táº¡o. \n
    #             Náº¿u cÃ¢u há»i má»i: {query} ÄÃ£ Äá»§ chá»§ ngá»¯ thÃ¬ káº¿t quáº£ tráº£ vá» váº«n lÃ  cÃ¢u há»i nÃ y: '{query}'. KhÃ´ng ÄÆ°á»£c tá»± suy luáº­n thiáº¿u khÃ¡ch quan."""
    # previous_query = ""
    # previous_answer = ""
    # if history != []:
    #     previous_query = history[-2].content
    #     previous_answer = history[-1].content
    # prompt = f"""Using Vietnamese, Create a SINGLE standalone question. Must be a question. If the new question: {query} already has a subject, the result should still be this question: '{query}'. Else If the new question lacks a subject, base on the previous query from user: '{previous_query}' and its answer '{previous_answer}'  to find the subject and add it to the new question '{query}' and return the created question .  
    # Only add the subject, do not add any other information."""
    # # query = query + " and "+  OpenAIGenerator().run(prompt)['replies'][0]
    # print(query)
    system_message =    ChatMessage.from_system(
            f"""Náº¿u cÃ¢u há»i thiáº¿u chá»§ ngá»¯ vÃ  ngá»¯ cáº£nh thÃ¬ cung cáº¥p thÃªm cho cÃ¢u há»i dá»±a vÃ o lá»ch sá»­ chat. Náº¿u thiáº¿u thÃ´ng tin cáº§n gá»i hÃ m Äá» láº¥y thÃªm thÃ´ng tin.
            Báº¡n chá» tráº£ lá»i dá»±a trÃªn thÃ´ng tin ÄÆ°á»£c cung cáº¥p, khÃ´ng ÄÆ°á»£c tá»± láº¥y thÃ´ng tin ngoÃ i Äá» tráº£ lá»i cho user.
            Náº¿u khÃ´ng cÃ³ yÃªu cáº§u chuyá»n ngÃ´n ngá»¯ tá»« user, thÃ¬ luÃ´n tráº£ lá»i báº±ng tiáº¿ng viá»t. Náº¿u ngÃ´n ngá»¯ cá»§a user lÃ  tiáº¿ng viá»t
            thÃ¬ luÃ´n tráº£ lá»i báº±ng tiáº¿ng Viá»t.
            Cáº§n Äá»nh dáº¡ng hÃ¬nh thá»©c cÃ¢u tráº£ lá»i sao cho rÃµ rÃ ng vÃ  Äáº¹p. """
        )
    history_temp = history
    history.append(system_message)
    # history.append(ChatMessage.from_system(prompt))
    history.append(ChatMessage.from_user(query))
    history_temp.append(ChatMessage.from_user(query))
    # history.append(ChatMessage.from_user(query))
    response = chat_generator.run(messages=history, generation_kwargs={"tools": tools})
    # print(response)
    # print(history)
    if response and response["replies"][0].meta["finish_reason"] == "tool_calls":
        function_call = json.loads(response["replies"][0].content)[0]
        function_name = function_call["function"]["name"]
        function_args = json.loads(function_call["function"]["arguments"])
        if (function_name== "chatbot_pipeline"):
            function_args.update({'history': history})
        # print("Function Name:", function_name)
        # print("Function Arguments:", function_args)
        ## Find the correspoding function and call it with the given arguments
        available_functions = {"chatbot_pipeline": chatbot_pipeline}
        function_to_call = available_functions[function_name]
        function_response = function_to_call(**function_args)
        for event in  function_response:
            # if "content" in event.choices[0].delta:
            current_response = event.choices[0].delta.content
            if current_response is not None :
                yield current_response
    else:
        # print("No tools.")
        # for event in  chatbot_pipeline(response['replies'][0].content,history):
        #     # if "content" in event.choices[0].delta:
        #     current_response = event.choices[0].delta.content
        #     if current_response is not None :
        #         yield current_response
        history.append(ChatMessage.from_user(query))
        for event in  chat_generator.client.chat.completions.create(
                model=chat_generator.model,
                messages=[mess.to_openai_format() for mess in history],            
                stream=True ):
            # if "content" in event.choices[0].delta:
            current_response = event.choices[0].delta.content
            if current_response is not None :
                yield current_response



# def chat_pipeline(question:str, history = []):
#     document_store = load_store()
#     pipeline = Pipeline()
#     pipeline.add_component("embedder", CohereTextEmbedder(model=model_name))
#     pipeline.add_component("retriever", QdrantEmbeddingRetriever(document_store=document_store))
#     pipeline.add_component("prompt_builder", DynamicChatPromptBuilder(runtime_variables=["query", "documents"]))
#     # pipeline.add_component("llm", OpenAIChatGenerator(model= "gpt-3.5-turbo"))
#     pipeline.connect("embedder.embedding", "retriever.query_embedding")
#     pipeline.connect("retriever.documents", "prompt_builder.documents")
#     # pipeline.connect("prompt_builder.prompt", "llm.messages")

#     system_message =    ChatMessage.from_system(
#             f"""Náº¿u cung cáº¥p thÃ´ng tin vá» khoÃ¡ há»c thÃ¬ cáº§n thÃªm web link (ÄÆ°á»ng dáº«n web) kÃ¨m theo. Tráº£ lá»i ngáº¯n gá»n Äá»§ Ã½. KhÃ´ng ÄÆ°á»£c tá»± suy luáº­n thiáº¿u thÃ´ng tin tá»« dá»¯ liá»u chat.
#             ThÃªm ngá»¯ cáº£nh cho cÃ¢u há»i dá»±a vÃ o cÃ¡c cÃ¢u há»i trÆ°á»c cá»§a user. Náº¿u thiáº¿u thÃ´ng tin cáº§n gá»i hÃ m Äá» láº¥y thÃªm thÃ´ng tin.
#             Báº¡n chá» tráº£ lá»i dá»±a trÃªn thÃ´ng tin ÄÆ°á»£c cung cáº¥p, khÃ´ng ÄÆ°á»£c tá»± láº¥y thÃ´ng tin ngoÃ i Äá» tráº£ lá»i cho user.
#             Náº¿u khÃ´ng cÃ³ yÃªu cáº§u chuyá»n ngÃ´n ngá»¯ tá»« user, thÃ¬ luÃ´n tráº£ lá»i báº±ng tiáº¿ng viá»t. Náº¿u ngÃ´n ngá»¯ cá»§a user lÃ  tiáº¿ng viá»t
#             thÃ¬ luÃ´n tráº£ lá»i báº±ng tiáº¿ng Viá»t.
#             Cáº§n Äá»nh dáº¡ng hÃ¬nh thá»©c cÃ¢u tráº£ lá»i sao cho rÃµ rÃ ng vÃ  Äáº¹p. """
#         )
    
#     history.append(system_message)
#     history.append(ChatMessage.from_user("""
#     Given these documents , answer the question.\nDocuments:
#         {% for doc in documents %}
#             {{ doc.content }}
#         {% endfor %}

#         \nQuestion: {{query}}
#         \nAnswer:
#     """))
#     messages = pipeline.run(data={"embedder": {"text": question}, "prompt_builder": { "prompt_source": history, "query": question}})['prompt_builder']['prompt']
#     chat_generator = OpenAIChatGenerator(model= "gpt-3.5-turbo")
#     return chat_generator.client.chat.completions.create(
#                 model=chat_generator.model,
#                 messages=[mess.to_openai_format() for mess in messages],            
#                 stream=True )



# def prompt_pipeline(question:str, history = []):
#     document_store = load_store()
#     pipeline = Pipeline()
#     pipeline.add_component("embedder", CohereTextEmbedder(model=model_name))
#     pipeline.add_component("retriever", QdrantEmbeddingRetriever(document_store=document_store))
#     pipeline.add_component("prompt_builder", DynamicChatPromptBuilder(runtime_variables=["query", "documents"]))
#     # pipeline.add_component("llm", OpenAIChatGenerator(model= "gpt-3.5-turbo"))
#     pipeline.connect("embedder.embedding", "retriever.query_embedding")
#     pipeline.connect("retriever.documents", "prompt_builder.documents")
#     # pipeline.connect("prompt_builder.prompt", "llm.messages")
#     system_message =   ChatMessage.from_system(
#             f"""Náº¿u cung cáº¥p thÃ´ng tin vá» khoÃ¡ há»c thÃ¬ cáº§n thÃªm web link (ÄÆ°á»ng dáº«n web) kÃ¨m theo. Tráº£ lá»i ngáº¯n gá»n Äá»§ Ã½. KhÃ´ng ÄÆ°á»£c tá»± suy luáº­n thiáº¿u thÃ´ng tin tá»« dá»¯ liá»u chat.
#             ThÃªm ngá»¯ cáº£nh cho cÃ¢u há»i dá»±a vÃ o cÃ¡c cÃ¢u há»i trÆ°á»c cá»§a user. Náº¿u thiáº¿u thÃ´ng tin cáº§n gá»i hÃ m Äá» láº¥y thÃªm thÃ´ng tin.
#             Báº¡n chá» tráº£ lá»i dá»±a trÃªn thÃ´ng tin ÄÆ°á»£c cung cáº¥p, khÃ´ng ÄÆ°á»£c tá»± láº¥y thÃ´ng tin ngoÃ i Äá» tráº£ lá»i cho user.
#             Náº¿u khÃ´ng cÃ³ yÃªu cáº§u chuyá»n ngÃ´n ngá»¯ tá»« user, thÃ¬ luÃ´n tráº£ lá»i báº±ng tiáº¿ng viá»t. Náº¿u ngÃ´n ngá»¯ cá»§a user lÃ  tiáº¿ng viá»t
#             thÃ¬ luÃ´n tráº£ lá»i báº±ng tiáº¿ng Viá»t.
#             Cáº§n Äá»nh dáº¡ng hÃ¬nh thá»©c cÃ¢u tráº£ lá»i sao cho rÃµ rÃ ng vÃ  Äáº¹p. """
#         )
#     history.append(system_message)
#     history.append(ChatMessage.from_user("""
#     Given these documents , answer the question.\nDocuments:
#         {% for doc in documents %}
#             {{ doc.content }}
#         {% endfor %}

#         \nQuestion: {{query}}
#         \nAnswer:
#     """))

#     return pipeline.run(data={"embedder": {"text": question}, "prompt_builder": { "prompt_source": history, "query": question}})['prompt_builder']['prompt']


# def chatbot_with_fc_stream(question:str, history = []):
#     start = time.time()
#     document_store = load_store()
#     pipeline = Pipeline()
#     pipeline.add_component("embedder", CohereTextEmbedder(model=model_name))
#     pipeline.add_component("retriever", QdrantEmbeddingRetriever(document_store=document_store))
#     pipeline.add_component("prompt_builder", DynamicChatPromptBuilder(runtime_variables=["query", "documents"]))
#     # pipeline.add_component("llm", OpenAIChatGenerator(model= "gpt-3.5-turbo"))
#     pipeline.connect("embedder.embedding", "retriever.query_embedding")
#     pipeline.connect("retriever.documents", "prompt_builder.documents")
#     # pipeline.connect("prompt_builder.prompt", "llm.messages")
#     system_message =    ChatMessage.from_system(
#             f"""Náº¿u thÃ´ng tin cÃ³ ÄÆ°á»ng dáº«n (web link) thÃ¬ pháº£i dÃ¹ng Äá» tráº£ lá»i, náº¿u khÃ´ng cÃ³ thÃ¬ khÃ´ng ÄÆ°á»£c tá»± Ã½ thÃªm báº­y. Tráº£ lá»i ngáº¯n gá»n Äá»§ Ã½. KhÃ´ng ÄÆ°á»£c tá»± suy luáº­n thiáº¿u thÃ´ng tin tá»« dá»¯ liá»u chat.
#             ThÃªm ngá»¯ cáº£nh cho cÃ¢u há»i dá»±a vÃ o cÃ¡c cÃ¢u há»i trÆ°á»c cá»§a user. Náº¿u thiáº¿u thÃ´ng tin cáº§n gá»i hÃ m Äá» láº¥y thÃªm thÃ´ng tin.
#             Báº¡n chá» tráº£ lá»i dá»±a trÃªn thÃ´ng tin ÄÆ°á»£c cung cáº¥p, khÃ´ng ÄÆ°á»£c tá»± láº¥y thÃ´ng tin ngoÃ i Äá» tráº£ lá»i cho user.
#             Náº¿u khÃ´ng cÃ³ yÃªu cáº§u chuyá»n ngÃ´n ngá»¯ tá»« user, thÃ¬ luÃ´n tráº£ lá»i báº±ng tiáº¿ng viá»t. Náº¿u ngÃ´n ngá»¯ cá»§a user lÃ  tiáº¿ng viá»t
#             thÃ¬ luÃ´n tráº£ lá»i báº±ng tiáº¿ng Viá»t.
#             Cáº§n Äá»nh dáº¡ng hÃ¬nh thá»©c cÃ¢u tráº£ lá»i sao cho rÃµ rÃ ng vÃ  Äáº¹p. """
#         )

    
#     history.append(system_message)
#     history.append(ChatMessage.from_user("""
#     Given these documents , answer the question.\nDocuments:
#         {% for doc in documents %}
#             {{ doc.content }}
#         {% endfor %}

#         \nQuestion: {{query}}
#         \nAnswer:
#     """))
#     messages = pipeline.run(data={"embedder": {"text": question}, "prompt_builder": { "prompt_source": history, "query": question}})['prompt_builder']['prompt']
#     chat_generator = OpenAIChatGenerator(model= "gpt-3.5-turbo")
#     for event in  chat_generator.client.chat.completions.create(
#                 model=chat_generator.model,
#                 messages=[mess.to_openai_format() for mess in messages],            
#                 stream=True ):
#         # if "content" in event.choices[0].delta:
#             current_response = event.choices[0].delta.content
#             if current_response is not None :
#                 yield current_response


# def chatbot_with_fc_stream4(message, messages=[]):
#     start = time.time()
#     if message == []:
#         messages.append(
#             ChatMessage.from_system(
#                 "Náº¿u khÃ´ng cÃ³ yÃªu cáº§u chuyá»n ngÃ´n ngá»¯ tá»« user, thÃ¬ luÃ´n tráº£ lá»i báº±ng tiáº¿ng viá»t. Náº¿u ngÃ´n ngá»¯ cá»§a user lÃ  tiáº¿ng viá»t thÃ¬ luÃ´n tráº£ lá»i báº±ng tiáº¿ng Viá»t. Báº¡n chá» tráº£ lá»i dá»±a trÃªn thÃ´ng tin ÄÆ°á»£c cung cáº¥p, khÃ´ng ÄÆ°á»£c tá»± láº¥y thÃ´ng tin ngoÃ i Äá» tráº£ lá»i cho user. VÃ  Äá»nh dáº¡ng hÃ¬nh thá»©c tráº£ lá»i sao cho Äáº¹p."
#             )
#         )
    
#     chat_generator = OpenAIChatGenerator(model="gpt-3.5-turbo")
#                                         #  ,streaming_callback=lambda chunk: print(chunk.content, end="", flush=True))
#     # chat_generator = OpenAIChatGenerator(model="gpt-3.5-turbo")
#     messages.append(
#         ChatMessage.from_system(
#             f"""Náº¿u cung cáº¥p thÃ´ng tin vá» khoÃ¡ há»c thÃ¬ cáº§n thÃªm web link (ÄÆ°á»ng dáº«n web) kÃ¨m theo. Tráº£ lá»i ngáº¯n gá»n Äá»§ Ã½. KhÃ´ng ÄÆ°á»£c tá»± suy luáº­n thiáº¿u thÃ´ng tin tá»« dá»¯ liá»u chat.
#             ThÃªm ngá»¯ cáº£nh cho cÃ¢u há»i dá»±a vÃ o cÃ¡c cÃ¢u há»i trÆ°á»c cá»§a user. Náº¿u thiáº¿u thÃ´ng tin cáº§n gá»i hÃ m Äá» láº¥y thÃªm thÃ´ng tin.
#             Báº¡n chá» tráº£ lá»i dá»±a trÃªn thÃ´ng tin ÄÆ°á»£c cung cáº¥p, khÃ´ng ÄÆ°á»£c tá»± láº¥y thÃ´ng tin ngoÃ i Äá» tráº£ lá»i cho user.
#             Náº¿u khÃ´ng cÃ³ yÃªu cáº§u chuyá»n ngÃ´n ngá»¯ tá»« user, thÃ¬ luÃ´n tráº£ lá»i báº±ng tiáº¿ng viá»t. Náº¿u ngÃ´n ngá»¯ cá»§a user lÃ  tiáº¿ng viá»t
#             thÃ¬ luÃ´n tráº£ lá»i báº±ng tiáº¿ng Viá»t.
#             Cáº§n Äá»nh dáº¡ng hÃ¬nh thá»©c cÃ¢u tráº£ lá»i sao cho rÃµ rÃ ng vÃ  Äáº¹p. """
#         )
#     )
    
#     # response = chat_generator.run(messages=messages)

#     tools = [
#             {
#                 "type": "function",
#                 "function": {
#                     "name": "chat_pipeline",
#                     "description": "Get information",
#                     "parameters": {
#                         "type": "object",
#                         "properties": {
#                             "query": {
#                                 "type": "string",
#                                 "description": "The query to use in the search. Infer this from the user's message. It should be a question or a statement. Add context for the query.",
#                             }
#                         },
#                         "required": ["query"],
#                     },
#                 },
#             },
#             {
#                 "type": "function",
#                 "function": {
#                     "name": "get_career_skills",
#                     "description": "Get user's goal and current career and get user's goal and current skills",
#                     "parameters": {
#                         "type": "object",
#                         "properties": {
#                             "goal_career": {
#                                 "type": "string",
#                                 "description": "Name of user's goal career, e.g. Backend Developer, Business Analyst, Data Analysts, Data Engineer, Data Scientist, Database Administrator,Devops Engineer,Frontend Developer,Game Development,Mobile Developer",
#                             },
#                             "current_career": {
#                                 "type": "string",
#                                 "description": "Name of user's current career, e.g. Backend Developer, Business Analyst, Data Analysts, Data Engineer, Data Scientist, Database Administrator,Devops Engineer,Frontend Developer,Game Development,Mobile Developer",
#                             },
#                             "goal_skills": {
#                                 "type": "string",
#                                 "description": "List name of user's goal skills, e.g. power bi, ssis,sql server,mysql ,redis ,docker ,software product management,.net core framework ,github, object-oriented programming (oop) ,relational database management systems (rdbms),data visualization,data warehouse,graphql,java,javascript ,machine learning,data analysis,business intelligence ,r,python , sql,golang...",
#                             },
#                             "current_skills": {
#                                 "type": "string",
#                                 "description": "List name of user's current skills, e.g. power bi, ssis,sql server,mysql ,redis ,docker ,software product management,.net core framework ,github, object-oriented programming (oop) ,relational database management systems (rdbms),data visualization,data warehouse,graphql,java,javascript ,machine learning,data analysis,business intelligence ,r,python , sql,golang...",
#                             },
#                             "query": {
#                                 "type": "string",
#                                 "description": "The query to use in the search. Infer this from the user's message. It should be a question or a statement",
#                             },
#                         },
#                         "required": [
#                             "goal_career",
#                             "current_career",
#                             "goal_skills",
#                             "current_skills",
#                             "query",
#                         ],
#                     },
#                 },
#             },
#         ]
#     messages_temp = messages
#     messages.append(ChatMessage.from_user(message))

#     response = chat_generator.client.chat.completions.create(
#                 model=chat_generator.model,
#                 messages=[mess.to_openai_format() for mess in messages],
#                 tools=tools,
#                 tool_choice="auto")
        
#     response_message = response.choices[0].message
#     if dict(response_message).get('tool_calls'): 
#             # Which function call was invoked
#         function_called = response_message.tool_calls[0].function.name   
#             # Extracting the arguments
#         function_args  = json.loads(response_message.tool_calls[0].function.arguments)
#             # Function names
#         available_functions = {
#                         "chat_pipeline": chat_pipeline,
#                         "get_content_course": get_content_course,
#                         "get_career_skills": get_career_skills,
#                     }
            
#         fuction_to_call = available_functions[function_called]
#         if function_called =="chat_pipeline":
#             for event in fuction_to_call(*list(function_args.values()),messages):
#         # if "content" in event.choices[0].delta:
#                 current_response = event.choices[0].delta.content
#                 if current_response is not None :
#                     yield current_response

#         else:             
#             response_message = fuction_to_call(*list(function_args.values()),messages)

#             print(response_message)
#             messages_temp.append(ChatMessage.from_user(message))
#             for event in chat_generator.client.chat.completions.create(
#                     model=chat_generator.model,
#                     messages=[mess.to_openai_format() for mess in messages_temp],            
#                     stream=True ):
#             # if "content" in event.choices[0].delta:
#                 current_response = event.choices[0].delta.content
#                 if current_response is not None :
#                     yield current_response
#     else:
#         # messages.pop()
#         # print(rag_pipeline_func(message))
#         # messages.append(
#         #             ChatMessage.from_function(
#         #                 content=json.dumps(rag_pipeline_func(message)), name="rag_pipeline_func"
#         #             )
#         #         )  
#         # messages.append(ChatMessage.from_user(message))
#         # messages = prompt_pipeline(message,messages)
#         for event in  chat_generator.client.chat.completions.create(
#                 model=chat_generator.model,
#                 messages=[mess.to_openai_format() for mess in messages],            
#                 stream=True ):
#         # if "content" in event.choices[0].delta:
#             current_response = event.choices[0].delta.content
#             if current_response is not None :
#                 yield current_response
    
#     end = time.time()
#     print("chat time: ",end - start)

    




# def chatbot_with_fc_stream1(question:str, history = []):
#     start = time.time()

#     prompt= f"""Using Vietnames. Create a SINGLE standalone question. The question should be based on the New question plus the Chat history. 
#     If the New question can stand on its own you should return the New question {question}. New question: \"{question}\", Chat history: \"{history}\"."""
#     history.append(ChatMessage.from_system(prompt))
#     chat_generator = OpenAIChatGenerator()
#     question = chat_generator.run(history)['replies'][0].content
#     print(question)
#     history.pop()
#     messages = prompt_pipeline(question,history)
#     chat_generator = OpenAIChatGenerator()
#     for event in  chat_generator.client.chat.completions.create(
#                 model=chat_generator.model,
#                 messages=[mess.to_openai_format() for mess in messages],            
#                 stream=True ):
#         # if "content" in event.choices[0].delta:
#             current_response = event.choices[0].delta.content
#             if current_response is not None :
#                 yield current_response

#     print("Chat time: ", time.time()- start)